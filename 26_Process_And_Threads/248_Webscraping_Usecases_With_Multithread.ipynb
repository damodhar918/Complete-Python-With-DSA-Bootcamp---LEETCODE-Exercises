{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be4d7b91",
   "metadata": {},
   "source": [
    "## 1. Simple Multi-Threaded Web Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49e5926c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "MULTI-THREADED WEB SCRAPER\n",
      "==================================================\n",
      "Fetching http://example.com/page0...\n",
      "Fetching http://example.com/page1...\n",
      "Fetching http://example.com/page2...\n",
      "Fetching http://example.com/page3...\n",
      "Fetching http://example.com/page4...\n",
      "Fetching http://example.com/page5...Fetching http://example.com/page6...\n",
      "Fetching http://example.com/page7...\n",
      "Fetching http://example.com/page8...\n",
      "Fetching http://example.com/page9...\n",
      "\n",
      "\n",
      "Scraped 10 pages in 2.02s\n",
      "Sequential would take ~10s\n",
      "Speedup: 4.9x\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import requests\n",
    "from typing import List\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MULTI-THREADED WEB SCRAPER\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simulated fetch (use real URLs in practice)\n",
    "def fetch_page(url):\n",
    "    \"\"\"Fetch and process a single URL\"\"\"\n",
    "    print(f\"Fetching {url}...\")\n",
    "    # In real code: response = requests.get(url, timeout=5)\n",
    "    time.sleep(1)  # Simulate network delay\n",
    "    return {\n",
    "        'url': url,\n",
    "        'status': 200,\n",
    "        'content_length': 5000\n",
    "    }\n",
    "\n",
    "def scrape_urls(urls: List[str], max_workers: int = 5):\n",
    "    \"\"\"Scrape multiple URLs concurrently\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(fetch_page, url) for url in urls]\n",
    "        \n",
    "        for future in futures:\n",
    "            try:\n",
    "                result = future.result(timeout=10)\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    return results, elapsed\n",
    "\n",
    "# Test\n",
    "urls = [f'http://example.com/page{i}' for i in range(10)]\n",
    "results, elapsed = scrape_urls(urls, max_workers=5)\n",
    "\n",
    "print(f\"\\nScraped {len(results)} pages in {elapsed:.2f}s\")\n",
    "print(f\"Sequential would take ~{len(urls)}s\")\n",
    "print(f\"Speedup: {len(urls) / elapsed:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c64a339",
   "metadata": {},
   "source": [
    "## 2. Scraper with Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961fe654",
   "metadata": {},
   "source": [
    "import threading\n",
    "from queue import Queue\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SCRAPER WITH QUEUE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class QueueBasedScraper:\n",
    "    def __init__(self, num_workers=5):\n",
    "        self.url_queue = Queue()\n",
    "        self.results = []\n",
    "        self.lock = threading.Lock()\n",
    "        self.num_workers = num_workers\n",
    "    \n",
    "    def fetch_url(self, url):\n",
    "        print(f\"Fetching {url}...\")\n",
    "        time.sleep(1)\n",
    "        return {'url': url, 'status': 200}\n",
    "    \n",
    "    def worker(self):\n",
    "        while True:\n",
    "            url = self.url_queue.get()\n",
    "            if url is None:  # Poison pill\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                result = self.fetch_url(url)\n",
    "                with self.lock:\n",
    "                    self.results.append(result)\n",
    "            finally:\n",
    "                self.url_queue.task_done()\n",
    "    \n",
    "    def scrape(self, urls):\n",
    "        # Add URLs to queue\n",
    "        for url in urls:\n",
    "            self.url_queue.put(url)\n",
    "        \n",
    "        # Create workers\n",
    "        threads = []\n",
    "        for _ in range(self.num_workers):\n",
    "            t = threading.Thread(target=self.worker)\n",
    "            t.start()\n",
    "            threads.append(t)\n",
    "        \n",
    "        # Wait for queue to empty\n",
    "        self.url_queue.join()\n",
    "        \n",
    "        # Add poison pills\n",
    "        for _ in range(self.num_workers):\n",
    "            self.url_queue.put(None)\n",
    "        \n",
    "        # Wait for workers\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "# Test\n",
    "scraper = QueueBasedScraper(num_workers=3)\n",
    "urls = [f'http://example.com/page{i}' for i in range(6)]\n",
    "results = scraper.scrape(urls)\n",
    "print(f\"\\nScraped {len(results)} pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aebe89",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c771a17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SCRAPER WITH QUEUE\n",
      "==================================================\n",
      "Fetching http://example.com/page0...\n",
      "Fetching http://example.com/page1...\n",
      "Fetching http://example.com/page2...\n",
      "Fetching http://example.com/page3...\n",
      "Fetching http://example.com/page4...\n",
      "Fetching http://example.com/page5...\n",
      "\n",
      "Scraped 6 pages\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "from queue import Queue\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SCRAPER WITH QUEUE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class QueueBasedScraper:\n",
    "    def __init__(self, num_workers=5):\n",
    "        self.url_queue = Queue()\n",
    "        self.results = []\n",
    "        self.lock = threading.Lock()\n",
    "        self.num_workers = num_workers\n",
    "    \n",
    "    def fetch_url(self, url):\n",
    "        print(f\"Fetching {url}...\")\n",
    "        time.sleep(1)\n",
    "        return {'url': url, 'status': 200}\n",
    "    \n",
    "    def worker(self):\n",
    "        while True:\n",
    "            url = self.url_queue.get()\n",
    "            if url is None:  # Poison pill\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                result = self.fetch_url(url)\n",
    "                with self.lock:\n",
    "                    self.results.append(result)\n",
    "            finally:\n",
    "                self.url_queue.task_done()\n",
    "    \n",
    "    def scrape(self, urls):\n",
    "        # Add URLs to queue\n",
    "        for url in urls:\n",
    "            self.url_queue.put(url)\n",
    "        \n",
    "        # Create workers\n",
    "        threads = []\n",
    "        for _ in range(self.num_workers):\n",
    "            t = threading.Thread(target=self.worker)\n",
    "            t.start()\n",
    "            threads.append(t)\n",
    "        \n",
    "        # Wait for queue to empty\n",
    "        self.url_queue.join()\n",
    "        \n",
    "        # Add poison pills\n",
    "        for _ in range(self.num_workers):\n",
    "            self.url_queue.put(None)\n",
    "        \n",
    "        # Wait for workers\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "# Test\n",
    "scraper = QueueBasedScraper(num_workers=3)\n",
    "urls = [f'http://example.com/page{i}' for i in range(6)]\n",
    "results = scraper.scrape(urls)\n",
    "print(f\"\\nScraped {len(results)} pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be11dba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b745d5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c195a40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c7df61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86618f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa2a64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556e282e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219df05d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebf83f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70f2cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3b9bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9715c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d99d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de40e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d9de39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd375fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092a13ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a8c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176bf959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde2863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7605db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17eb09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be08b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4289f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ee986b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a8ad80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f1c653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50418549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a5c0e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0266a5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39b1c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b6c483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d75926a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f067e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851995a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
